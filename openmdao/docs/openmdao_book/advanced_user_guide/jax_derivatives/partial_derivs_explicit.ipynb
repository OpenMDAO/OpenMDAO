{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input",
     "active-ipynb",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from openmdao.utils.notebook_utils import notebook_mode\n",
    "except ImportError:\n",
    "    !python -m pip install openmdao[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Partial Derivatives of Explicit Components Using JAX\n",
    "\n",
    "One of the barriers to using OpenMDAO is that to truly take advantage of OpenMDAO, the user needs to write code for the analytic partial derivatives of their `Components`. To avoid that, users can use the optional third-party [JAX](https://jax.readthedocs.io/en/latest/index.html) library, which can automatically differentiate native Python and NumPy functions.  \n",
    "\n",
    "This notebook gives an example of using JAX to do automatic differentiation (AD) for the Sellar example. Only [forward mode AD](https://jax.readthedocs.io/en/latest/_autosummary/jax.jacfwd.html#jax.jacfwd) will be used in this example, however other options are:\n",
    "\n",
    "- [reverse-mode AD](https://jax.readthedocs.io/en/latest/_autosummary/jax.jacrev.html#jax.jacrev)\n",
    "- [forward-mode vector-Jacobian product](https://jax.readthedocs.io/en/latest/_autosummary/jax.vjp.html#jax.vjp)\n",
    "- [reverse-mode Jacobian-vector product](https://jax.readthedocs.io/en/latest/_autosummary/jax.jvp.html#jax.jvp)\n",
    "\n",
    "Forward-mode is better for \"tall\" Jacobian matrices (more outputs than inputs) whereas reverse-mode is better for \"wide\" Jacobian matrices (more inputs than outputs).\n",
    "\n",
    "This notebook also shows how to use JAX's just-in-time (jit) compiling capabilities to dramatically speed up computations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of JAX is optional for OpenMDAO so if not already installed, the user needs to install it. See the [installation instructions](https://github.com/google/jax#installation) for more information about installing JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some standard OpenMDAO imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import openmdao.api as om"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JAX library includes a NumPy-like API, `jax.numpy`, which implements the NumPy API using the primitives in JAX. Almost anything that can be done with NumPy can be done with `jax.numpy`. JAX arrays are similar to NumPy arrays, but they are designed to work with accelerators such as GPUs and TPU. \n",
    "\n",
    "To use `jax.numpy`, it needs to be imported, using the commonly used `jnp` abbreviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default for JAX is to do single precision computations. For this example, we want to use double precision, so this line of code is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one of the `ExplicitComponent`s in the model where derivatives will be computed using JAX. Comments interspersed in the code provide some explanations and guidance. Here is an overview of the steps that need to be taken to make use of JAX for your `ExplicitComponent`. \n",
    "\n",
    "1. Write a method to compute the outputs from the inputs. Borrowing from AD terminology, a suggested name for this method is `_compute_primal`. This method is the same as what you would normally write for the `compute` method of an `ExplicitComponent`, but it takes as its arguments the actual individual input variables rather than a dictionary of the inputs. This allows us to use JAX's AD capabilities on this method. The `_compute_primal` method simply returns the outputs as a single value or as a tuple if there are more than one outputs. Apply the `jit` decorator to this method to speed up the computations.\n",
    "2. In the constructor of the `ExplicitComponent`, create an attribute and assign to it a function that will compute the partial derivatives of the `ExplicitComponent`. This simply makes use of the JAX function, `jacfwd`, applied to the `_compute_primal` method.\n",
    "3. Create a method that computes the partials. In this example, it is called `_compute_partials_jacfwd` but could be any name that makes sense to the user. Apply the `jit` decorator to this method to speed up the computations.\n",
    "4. Make use of the `_compute_primal` method in the usual OpenMDAO `ExplicitComponent.compute` method.\n",
    "5. Make use of the `_compute_partials_jacfwd` in the usual OpenMDAO `ExplicitComponent.compute_partials` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SellarDis1(om.ExplicitComponent):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # argnums specifies which positional arguments to differentiate with respect to.\n",
    "        # Here we want derivates with respect to all 3 inputs of _compute_primal.\n",
    "        self.deriv_func_jacfwd = jax.jacfwd(self._compute_primal, argnums=[0, 1, 2])\n",
    "\n",
    "    def initialize(self):\n",
    "        # Added this option to this model to demonstrate how having options\n",
    "        # requires special care when using jit. See comments below\n",
    "        self.options.declare('scaling_ref', types=(float,), default=0.1)\n",
    "\n",
    "    def setup(self):\n",
    "        ref = self.options['scaling_ref']\n",
    "\n",
    "        # Global Design Variable\n",
    "        self.add_input('z', val=np.zeros(2))\n",
    "\n",
    "        # Local Design Variable\n",
    "        self.add_input('x', val=0.)\n",
    "\n",
    "        # Coupling parameter\n",
    "        self.add_input('y2', val=1.0)\n",
    "\n",
    "        # Coupling output\n",
    "        self.add_output('y1', val=1.0, lower=0.1, upper=1000., ref=ref)\n",
    "\n",
    "    def setup_partials(self):\n",
    "        # Finite difference everything\n",
    "        self.declare_partials('*', '*')\n",
    "\n",
    "    # The \"partial\" decorator returns a new function that has the same body as the original\n",
    "    # function, but with the specified arguments bound.\n",
    "    # Need to tell jit that self is a \"static\" argument. \n",
    "    # This allows the jitted class to access the options attribute\n",
    "    @partial(jax.jit, static_argnums=(0,)) \n",
    "    def _compute_primal(self, z, x, y2):\n",
    "        return z[0]**2 + z[1] + x - 0.2*y2\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def _compute_partials_jacfwd(self, z, x, y2):\n",
    "        # Always returns a tuple\n",
    "        dz, dx, dy2 = self.deriv_func_jacfwd(z, x, y2)\n",
    "        return dz, dx, dy2\n",
    "\n",
    "    def compute(self, inputs, outputs):\n",
    "        outputs['y1'] = self._compute_primal(*inputs.values())\n",
    "\n",
    "    def compute_partials(self, inputs, partials):\n",
    "        dz, dx, dy2 = self._compute_partials_jacfwd(*inputs.values())\n",
    "\n",
    "        partials['y1', 'z'] = dz\n",
    "        partials['y1', 'x'] = dx\n",
    "        partials['y1', 'y2'] = dy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the second Sellar `ExplicitComponent` should be written in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SellarDis2(om.ExplicitComponent):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.deriv_func_jacfwd = jax.jacfwd(self._compute_primal, argnums=[0, 1])\n",
    "\n",
    "    def setup(self):\n",
    "        # Global Design Variable\n",
    "        self.add_input('z', val=jnp.zeros(2))\n",
    "\n",
    "        # Coupling parameter\n",
    "        self.add_input('y1', val=1.0)\n",
    "\n",
    "        # Coupling output\n",
    "        self.add_output('y2', val=1.0, lower=0.1, upper=1000., ref=1.0)\n",
    "\n",
    "    def setup_partials(self):\n",
    "        self.declare_partials('*', '*')\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,) ) \n",
    "    def _compute_primal(self, z, y1):\n",
    "        # Depending on whether this is called via compute or compute_partials, y1 could have\n",
    "        # different dimensions. It's just a scalar though\n",
    "        if np.ndim(y1) == 1:\n",
    "            y1 = y1[0]\n",
    "\n",
    "        # if y1.real < 0.0:\n",
    "        #     y1 *= -1\n",
    "        # Because of jit, conditionals cannot be used as is, as in the above two lines of code. \n",
    "        # Fortunately, JAX provides control flow primitives to deal with that.\n",
    "        # For if statements, JAX provided the cond function.\n",
    "        # See https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#python-control-flow-jit\n",
    "        # for more information about control flow when using jit\n",
    "        y1 = jax.lax.cond(y1.real < 0.0, lambda y1 : -y1, lambda y1 : y1, y1)\n",
    "\n",
    "        return y1**.5 + z[0] + z[1]\n",
    "\n",
    "    @partial(jax.jit, static_argnums=(0,))\n",
    "    def _compute_partials_jacfwd(self, z, y1):\n",
    "        dz, dy1 = self.deriv_func_jacfwd(z, y1)\n",
    "        return dz, dy1\n",
    "\n",
    "    def compute(self, inputs, outputs):\n",
    "        outputs['y2'] = self._compute_primal(*inputs.values())\n",
    "\n",
    "    def compute_partials(self, inputs, partials):\n",
    "        # pass in y1, which is used in a conditional, as a scalar, which is hashable\n",
    "        z, y1 = inputs.values()\n",
    "        y1 = y1[0]\n",
    "        dz, dy1 = self._compute_partials_jacfwd(z, y1)\n",
    "\n",
    "        partials['y2', 'z'] = dz\n",
    "        partials['y2', 'y1'] = dy1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of this code is standard OpenMDAO code. The code can be run as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SellarDerivatives(om.Group):\n",
    "    \"\"\"\n",
    "    Group containing the Sellar MDA. This version uses the disciplines with derivatives.\n",
    "    \"\"\"\n",
    "\n",
    "    def setup(self):\n",
    "        self.add_subsystem('d1', SellarDis1(), promotes=['x', 'z', 'y1', 'y2'])\n",
    "        self.add_subsystem('d2', SellarDis2(), promotes=['z', 'y1', 'y2'])\n",
    "\n",
    "        obj = self.add_subsystem('obj_cmp', om.ExecComp('obj = x**2 + z[1] + y1 + exp(-y2)', obj=0.0,\n",
    "                                                  x=0.0, z=np.array([0.0, 0.0]), y1=0.0, y2=0.0),\n",
    "                           promotes=['obj', 'x', 'z', 'y1', 'y2'])\n",
    "\n",
    "        con1 = self.add_subsystem('con_cmp1', om.ExecComp('con1 = 3.16 - y1', con1=0.0, y1=0.0),\n",
    "                           promotes=['con1', 'y1'])\n",
    "        con2 = self.add_subsystem('con_cmp2', om.ExecComp('con2 = y2 - 24.0', con2=0.0, y2=0.0),\n",
    "                           promotes=['con2', 'y2'])\n",
    "\n",
    "        # manually declare partials to allow graceful fallback to FD when nested under a higher\n",
    "        # level complex step approximation.\n",
    "        obj.declare_partials(of='*', wrt='*', method='cs')\n",
    "        con1.declare_partials(of='*', wrt='*', method='cs')\n",
    "        con2.declare_partials(of='*', wrt='*', method='cs')\n",
    "\n",
    "        self.set_input_defaults('x', 1.0)\n",
    "        self.set_input_defaults('z', np.array([5.0, 2.0]))\n",
    "\n",
    "\n",
    "prob = om.Problem()\n",
    "prob.model = model = SellarDerivatives()\n",
    "\n",
    "model.add_design_var('z', lower=np.array([-10.0, 0.0]), upper=np.array([10.0, 10.0]))\n",
    "model.add_design_var('x', lower=0.0, upper=10.0)\n",
    "model.add_objective('obj')\n",
    "model.add_constraint('con1', upper=0.0)\n",
    "model.add_constraint('con2', upper=0.0)\n",
    "model.add_constraint('x', upper=11.0, linear=True)\n",
    "\n",
    "prob.set_solver_print(level=0)\n",
    "\n",
    "prob.driver = om.ScipyOptimizeDriver(optimizer='SLSQP', tol=1e-9, disp=False)\n",
    "\n",
    "prob.setup(check=False, mode='fwd')\n",
    "\n",
    "prob.run_driver()\n",
    "print(prob.get_val('obj'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove-input",
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from openmdao.utils.assert_utils import assert_near_equal\n",
    "assert_near_equal(prob['z'][0], 1.9776, 1e-2)\n",
    "assert_near_equal(prob['z'][1], 0.0, 1e-3)\n",
    "assert_near_equal(prob['x'], 0.0, 1e-3)\n",
    "\n",
    "with np.printoptions(linewidth=1024):\n",
    "    prob.check_partials(method='cs', compact_print=False);"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orphan": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
